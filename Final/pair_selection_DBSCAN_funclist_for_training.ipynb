{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from hurst import compute_Hc\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from arch.unitroot import engle_granger\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(price_df, etf_info, start_date, end_date):\n",
    "\n",
    "    price_df = price_df.astype({'Date': 'datetime64[ns]'})\n",
    "\n",
    "    formation = price_df[(price_df.Date>=start_date) & (price_df.Date<end_date)]\n",
    "    formation = formation.set_index('Date')\n",
    "\n",
    "    formation_etf_list = list(formation[formation.index == formation.index[0]].tic.unique())\n",
    "\n",
    "    close_df = pd.DataFrame(index = formation.index.unique())\n",
    "\n",
    "    for x in tqdm(formation_etf_list):\n",
    "        tmp = test = formation['Close'][formation.tic == x]\n",
    "        close_df = pd.concat([close_df, tmp], axis=1)\n",
    "\n",
    "    close_df.columns = formation_etf_list\n",
    "    rtn_df = close_df.pct_change()[1:]\n",
    "\n",
    "    etf_info['volume'] = etf_info['Avg. Daily Volume'].str.replace(',','').astype('float')\n",
    "    low_volume_etf = etf_info[etf_info.volume < etf_info.volume.quantile(0.5)].Symbol.to_list()\n",
    "\n",
    "    return formation, close_df, rtn_df, low_volume_etf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_return(rtn_df, pc_selecting_threshold):\n",
    "    rtn_df_scaled = pd.DataFrame(StandardScaler().fit_transform(rtn_df))\n",
    "    pca = PCA()\n",
    "    pca.fit(rtn_df_scaled)\n",
    "    cumsum_eigen_value = np.cumsum(pca.explained_variance_ratio_)\n",
    "    pca_components = pca.components_[:np.where(cumsum_eigen_value >= pc_selecting_threshold)[0][0]]\n",
    "\n",
    "    pc_rtn = pd.DataFrame(data=pca_components.T, index=rtn_df.columns)\n",
    "    pc_rtn = pc_rtn.add_prefix(\"P\")\n",
    "\n",
    "    return pc_rtn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(close_df, pc_rtn, eps, min_samples, cluster_size_limit, cluster_member_counts):\n",
    "    \n",
    "    data = StandardScaler().fit_transform(pc_rtn)\n",
    "    clf = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clf.fit(data)\n",
    "\n",
    "    labels = clf.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(\"Clusters discovered: %d\" % n_clusters_)\n",
    "\n",
    "    clustered = clf.labels_\n",
    "    clustered_series = pd.Series(index = pc_rtn.index, data = clustered.flatten())\n",
    "    clustered_series_all = pd.Series(index = pc_rtn.index, data = clustered.flatten())\n",
    "    clustered_series = clustered_series[clustered_series != -1]\n",
    "\n",
    "    counts = clustered_series.value_counts()\n",
    "    ticker_count_reduced = counts[(counts > 1) & (counts <= cluster_size_limit)]\n",
    "    print(\"Clusters formed: %d\" % len(ticker_count_reduced))\n",
    "    print(\"Pairs to evaluate: %d\" % (ticker_count_reduced*(ticker_count_reduced-1)).sum())\n",
    "\n",
    "\n",
    "    # 클러스터링 안에 etf가 너무 많으면 pair selection 시간 너무 많이 걸림. 너무 많은 etf 포함하고 있는 clustering 제외하기\n",
    "    # cluster_memeber_counts 파라미터 조정 필요. 너무 크면 돌리는데 시간이 오래 걸리고, 너무 작으면 페어가 안뽑힐 수도 있음. \n",
    "    counts = clustered_series.value_counts()\n",
    "    clusters_viz_list = list(counts[(counts < cluster_member_counts) & (counts > 1)].index)[::-1]\n",
    "    print('final_clusters index : ', clusters_viz_list)\n",
    "\n",
    "    return clusters_viz_list, clustered_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pair_selection(close_df, rtn_df, low_volume_etf, clusters_viz_list, clustered_series, inverse_threshold, coint_pvalue_threshold, hurst_threshold, half_life_threshold, mean_reverting_freq):\n",
    "    \n",
    "    selected_pair = []\n",
    "    for i in tqdm(range(len(clusters_viz_list))):\n",
    "        test_list = list(clustered_series[clustered_series == clusters_viz_list[i]].index)\n",
    "\n",
    "        # 1. 거래량 10분위 이하 제거\n",
    "        vol_screened_test_list = []\n",
    "        for x in test_list:\n",
    "            if (x in low_volume_etf) == False:\n",
    "                vol_screened_test_list.append(x)       \n",
    "                \n",
    "        # 2. corr -0.99 보다 작은 etf가 존재하는지 -> 숏포지션 잡을 수 있는지 여부\n",
    "        short_avail_test_list = []\n",
    "        short_pair = {}\n",
    "        for x in vol_screened_test_list:\n",
    "            if rtn_df.corr()[x].min() < inverse_threshold:\n",
    "                short_avail_test_list.append(x)\n",
    "                short_pair[x] = rtn_df.corr()[x].idxmin()\n",
    "                \n",
    "        # 3. Cointegration - pvalue 0.05 이하인 pair sorting\n",
    "        log_price_data = np.log(close_df[short_avail_test_list])\n",
    "        comb = list(combinations(short_avail_test_list, 2))\n",
    "        eg_pvalue = {}\n",
    "\n",
    "        for x in tqdm(comb):\n",
    "            score, pvalue, _ = coint(log_price_data[x[0]], log_price_data[x[1]], method='aeg')\n",
    "            eg_pvalue[x] = pvalue\n",
    "\n",
    "        possible_pair = [x[0] for x in sorted(eg_pvalue.items(), key=lambda item: item[1]) if x[1] <= coint_pvalue_threshold]\n",
    "\n",
    "        # 4. Hurst exponent test - mean reverting intensity : H < 0.5 이하인 종목 선정\n",
    "        spread_df = pd.DataFrame(index = log_price_data.index)\n",
    "\n",
    "        for x in possible_pair:\n",
    "            spread_df[x] = close_df[x[0]] - close_df[x[1]]\n",
    "        \n",
    "        hurst_screened_list = []\n",
    "\n",
    "        for cnd in possible_pair:\n",
    "            if compute_Hc(spread_df[cnd])[0] <= hurst_threshold:\n",
    "                hurst_screened_list.append(cnd)\n",
    "\n",
    "        spread_df = spread_df[hurst_screened_list]\n",
    "\n",
    "        # 5. Half life가 trading period 안에 들어오는 종목 스크리닝\n",
    "        spread_df_lag = spread_df.shift(1)\n",
    "        spread_df_diff = spread_df - spread_df_lag\n",
    "\n",
    "        hl_screened_list = []\n",
    "        for i in range(len(hurst_screened_list)):\n",
    "            X = sm.add_constant(spread_df_lag.iloc[1:,i])\n",
    "            model = sm.OLS(spread_df_diff.iloc[1:,i], X)\n",
    "            result = model.fit()\n",
    "            lamda = result.params[1]\n",
    "            HL = -np.log(2) / lamda\n",
    "\n",
    "        # trading period 고려\n",
    "            if (HL <= half_life_threshold) & (HL >= 1):\n",
    "                hl_screened_list.append(hurst_screened_list[i])\n",
    "\n",
    "        spread_df = spread_df[hl_screened_list]\n",
    "\n",
    "        # 6. mean-reverting freq이 충분히 자주 발생하는지 스크리닝\n",
    "        freq_screened_list = []\n",
    "\n",
    "        for j in range(len(spread_df.columns)):\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            for i in range(len(spread_df.index)):\n",
    "\n",
    "                if i == len(spread_df.index) -1:\n",
    "                    break\n",
    "\n",
    "                elif spread_df.iloc[i+1,j] >= spread_df.iloc[i,j]:\n",
    "                    if (spread_df.mean()[j] >= spread_df.iloc[i,j]) & (spread_df.mean()[j] <= spread_df.iloc[i+1,j]):\n",
    "                        cnt += 1\n",
    "\n",
    "                else:\n",
    "                    if (spread_df.mean()[j] >= spread_df.iloc[i+1,j]) & (spread_df.mean()[j] <= spread_df.iloc[i,j]):\n",
    "                        cnt += 1\n",
    "            \n",
    "            if cnt >= mean_reverting_freq:\n",
    "                freq_screened_list.append(hl_screened_list[j])\n",
    "\n",
    "        selected_pair.append(freq_screened_list)\n",
    "\n",
    "    return selected_pair, short_pair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('quant')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7b6214bfe3ce19b2336b5ee82fe9739f68968628bd5eb86b17deae08d37c2fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
