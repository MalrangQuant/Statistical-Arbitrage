{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from hurst import compute_Hc\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from arch.unitroot import engle_granger\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pair_select:\n",
    "\n",
    "    def __init__ (self, price_df, etf_info, formation_start_date, formation_end_date, trade_start_date, trade_end_date):\n",
    "        self.price_df               = price_df                  # Price History\n",
    "        self.etf_info               = etf_info                  # etf_info\n",
    "        self.formation_start_date   = formation_start_date      \n",
    "        self.formation_end_date     = formation_end_date        \n",
    "        self.trade_start_date       = trade_start_date           \n",
    "        self.trade_end_date         = trade_end_date        \n",
    "    \n",
    "        _, self.close_df_test, self.rtn_df_test, self.low_volume_etf_test \\\n",
    "            = self.preprocessing(trade_start_date, trade_end_date)\n",
    "\n",
    "        self.preprocessing(self.formation_start_date, self.formation_end_date)\n",
    "        self.get_clustering()\n",
    "        self.get_pair_selection()\n",
    "\n",
    "    def preprocessing (self, start_date, end_date):\n",
    "        '''For formation, input formation period'''\n",
    "        \n",
    "        formation = self.price_df[(self.price_df.Date >= start_date) & (self.price_df.Date < end_date)].set_index('Date')\n",
    "        #formation_etf_list = list(formation[formation.index == formation.index[0]].tic.unique())\n",
    "        formation_etf_list = list(formation[formation.index == formation.index[0]].tic.unique())\n",
    "        \n",
    "        close_df = pd.DataFrame(index = formation.index.unique())\n",
    "\n",
    "        for x in tqdm(formation_etf_list):\n",
    "            tmp = formation['Close'][formation.tic == x]\n",
    "            close_df = pd.concat([close_df, tmp], axis=1)\n",
    "\n",
    "        close_df.columns = formation_etf_list\n",
    "        #rtn_df = close_df.pct_change()[1:] # return을 pct로?\n",
    "        rtn_df = np.log(close_df).diff()[1:]\n",
    "\n",
    "        etf_info['volume'] = etf_info['Avg. Daily Volume'].str.replace(',','').astype('float')\n",
    "        low_volume_etf = etf_info[etf_info.volume < etf_info.volume.quantile(0.1)].Symbol.to_list()\n",
    "\n",
    "        self.formation       = formation\n",
    "        self.close_df        = close_df\n",
    "        self.rtn_df          = rtn_df\n",
    "        self.low_volume_etf  = low_volume_etf\n",
    "\n",
    "        return formation, close_df, rtn_df, low_volume_etf\n",
    "\n",
    "    def get_pca_return(self, pc_selecting_threshold=0.9):\n",
    "\n",
    "        rtn_df_scaled = pd.DataFrame(StandardScaler().fit_transform(self.rtn_df))\n",
    "        pca = PCA()\n",
    "        pca.fit(rtn_df_scaled)\n",
    "        cumsum_eigen_value = np.cumsum(pca.explained_variance_ratio_)\n",
    "        pca_components = pca.components_[:np.where(cumsum_eigen_value >= pc_selecting_threshold)[0][0]]\n",
    "\n",
    "        pc_rtn = pd.DataFrame(data=pca_components.T, index=self.rtn_df.columns)\n",
    "        pc_rtn = pc_rtn.add_prefix(\"P\")\n",
    "\n",
    "        self.pc_rtn = pc_rtn\n",
    "\n",
    "        return pc_rtn        \n",
    "\n",
    "    def get_clustering(self, pca_rtn=None, eps=1.8, min_samples=4, cluster_size_limit=100, cluster_member_counts=100):\n",
    "\n",
    "        if pca_rtn is not None:\n",
    "            pc_rtn = pca_rtn\n",
    "        else:\n",
    "            pc_rtn = self.get_pca_return()        \n",
    "\n",
    "        data = StandardScaler().fit_transform(pc_rtn)\n",
    "        clf = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        clf.fit(data)\n",
    "\n",
    "        labels = clf.labels_\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        print(\"Clusters discovered: %d\" % n_clusters_)\n",
    "\n",
    "        clustered = clf.labels_\n",
    "        clustered_series = pd.Series(index = pc_rtn.index, data = clustered.flatten())\n",
    "        clustered_series_all = pd.Series(index = pc_rtn.index, data = clustered.flatten())\n",
    "        clustered_series = clustered_series[clustered_series != -1]\n",
    "\n",
    "        counts = clustered_series.value_counts()\n",
    "        ticker_count_reduced = counts[(counts > 1) & (counts <= cluster_size_limit)]\n",
    "        print(\"Clusters formed: %d\" % len(ticker_count_reduced))\n",
    "        print(\"Pairs to evaluate: %d\" % (ticker_count_reduced*(ticker_count_reduced-1)).sum())\n",
    "\n",
    "\n",
    "        # 클러스터링 안에 etf가 너무 많으면 pair selection 시간 너무 많이 걸림. 너무 많은 etf 포함하고 있는 clustering 제외하기\n",
    "        # cluster_memeber_counts 파라미터 조정 필요. 너무 크면 돌리는데 시간이 오래 걸리고, 너무 작으면 페어가 안뽑힐 수도 있음. \n",
    "        counts = clustered_series.value_counts()\n",
    "        clusters_viz_list = list(counts[(counts < cluster_member_counts) & (counts > 1)].index)[::-1]\n",
    "        print('final_clusters index : ', clusters_viz_list)\n",
    "\n",
    "        self.clusters_viz_list = clusters_viz_list\n",
    "        self.clustered_series  = clustered_series\n",
    "\n",
    "        return clusters_viz_list, clustered_series\n",
    "\n",
    "    def get_pair_selection(self, inverse_threshold=0.99,\\\n",
    "                                 coint_pvalue_threshold=0.05,\\\n",
    "                                 hurst_threshold=0.5,\\\n",
    "                                 half_life_threshold=30,\\\n",
    "                                 mean_reverting_freq=12):\n",
    "\n",
    "        '''pair with list, short_pair with dict'''\n",
    "        selected_pair = []\n",
    "        for i in tqdm(range(len(self.clusters_viz_list))):\n",
    "            test_list = list(self.clustered_series[self.clustered_series == self.clusters_viz_list[i]].index)\n",
    "\n",
    "            # 1. 거래량 10분위 이하 제거\n",
    "            vol_screened_test_list = []\n",
    "            for x in test_list:\n",
    "                if (x in self.low_volume_etf) == False:\n",
    "                    vol_screened_test_list.append(x)       \n",
    "                    \n",
    "            # 2. corr -0.99 보다 작은 etf가 존재하는지 -> 숏포지션 잡을 수 있는지 여부\n",
    "            short_avail_test_list = []\n",
    "            short_pair = {}\n",
    "            for x in vol_screened_test_list:\n",
    "                if self.rtn_df.corr()[x].min() < inverse_threshold:\n",
    "                    short_avail_test_list.append(x)\n",
    "                    short_pair[x] = self.rtn_df.corr()[x].idxmin()\n",
    "                    \n",
    "            # 3. Cointegration - pvalue 0.05 이하인 pair sorting\n",
    "            log_price_data = np.log(self.close_df[short_avail_test_list])\n",
    "            comb = list(combinations(short_avail_test_list, 2))\n",
    "            eg_pvalue = {}\n",
    "\n",
    "            for x in tqdm(comb):\n",
    "                score, pvalue, _ = coint(log_price_data[x[0]], log_price_data[x[1]], method='aeg')\n",
    "                eg_pvalue[x] = pvalue\n",
    "\n",
    "            possible_pair = [x[0] for x in sorted(eg_pvalue.items(), key=lambda item: item[1]) if x[1] <= coint_pvalue_threshold]\n",
    "\n",
    "            # 4. Hurst exponent test - mean reverting intensity : H < 0.5 이하인 종목 선정\n",
    "            spread_df = pd.DataFrame(index = log_price_data.index)\n",
    "\n",
    "            for x in possible_pair:\n",
    "                spread_df[x] = self.close_df[x[0]] - self.close_df[x[1]]\n",
    "            \n",
    "            hurst_screened_list = []\n",
    "\n",
    "            for cnd in possible_pair:\n",
    "                if compute_Hc(spread_df[cnd])[0] <= hurst_threshold:\n",
    "                    hurst_screened_list.append(cnd)\n",
    "\n",
    "            spread_df = spread_df[hurst_screened_list]\n",
    "\n",
    "            # 5. Half life가 trading period 안에 들어오는 종목 스크리닝\n",
    "            spread_df_lag = spread_df.shift(1)\n",
    "            spread_df_diff = spread_df - spread_df_lag\n",
    "\n",
    "            hl_screened_list = []\n",
    "\n",
    "            for i in range(len(hurst_screened_list)):\n",
    "\n",
    "                X = sm.add_constant(spread_df_lag.iloc[1:,i])\n",
    "                model = sm.OLS(spread_df_diff.iloc[1:,i], X)\n",
    "                result = model.fit()\n",
    "                lamda = result.params[1]\n",
    "                HL = -np.log(2) / lamda\n",
    "\n",
    "            # trading period 고려\n",
    "                if (HL <= half_life_threshold) & (HL >= 1):\n",
    "                    hl_screened_list.append(hurst_screened_list[i])\n",
    "\n",
    "            spread_df = spread_df[hl_screened_list]\n",
    "\n",
    "            # 6. mean-reverting freq이 충분히 자주 발생하는지 스크리닝\n",
    "            freq_screened_list = []\n",
    "\n",
    "            for j in range(len(spread_df.columns)):\n",
    "\n",
    "                cnt = 0\n",
    "\n",
    "                for i in range(len(spread_df.index)):\n",
    "\n",
    "                    if i == len(spread_df.index) -1:\n",
    "                        break\n",
    "\n",
    "                    elif spread_df.iloc[i+1,j] >= spread_df.iloc[i,j]:\n",
    "                        if (spread_df.mean()[j] >= spread_df.iloc[i,j]) & (spread_df.mean()[j] <= spread_df.iloc[i+1,j]):\n",
    "                            cnt += 1\n",
    "\n",
    "                    else:\n",
    "                        if (spread_df.mean()[j] >= spread_df.iloc[i+1,j]) & (spread_df.mean()[j] <= spread_df.iloc[i,j]):\n",
    "                            cnt += 1\n",
    "                \n",
    "                if cnt >= mean_reverting_freq:\n",
    "                    freq_screened_list.append(hl_screened_list[j])\n",
    "\n",
    "            selected_pair.append(freq_screened_list)\n",
    "\n",
    "        self.selected_pair = selected_pair[0]\n",
    "        self.short_pair = short_pair\n",
    "\n",
    "        return selected_pair[0], short_pair\n",
    "\n",
    "    #def \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:32<00:00, 49.17it/s]\n",
      "100%|██████████| 1532/1532 [00:34<00:00, 43.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters discovered: 18\n",
      "Clusters formed: 17\n",
      "Pairs to evaluate: 2000\n",
      "final_clusters index :  [17, 16, 12, 15, 5, 13, 2, 14, 3, 7, 6, 11, 1, 8, 9, 10, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 123.59it/s]\n",
      "  0%|          | 0/17 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Series length must be greater or equal to 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_start_date  \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2021-04-01\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_end_date    \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2021-04-30\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ps \u001b[39m=\u001b[39m pair_select(price_df, etf_info, start_date, end_date, test_start_date, test_end_date)\n",
      "\u001b[1;32m/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb 셀 3\u001b[0m in \u001b[0;36mpair_select.__init__\u001b[0;34m(self, price_df, etf_info, formation_start_date, formation_end_date, trade_start_date, trade_end_date)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformation_start_date, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformation_end_date)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clustering()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_pair_selection()\n",
      "\u001b[1;32m/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb 셀 3\u001b[0m in \u001b[0;36mpair_select.get_pair_selection\u001b[0;34m(self, inverse_threshold, coint_pvalue_threshold, hurst_threshold, half_life_threshold, mean_reverting_freq)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m hurst_screened_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39mfor\u001b[39;00m cnd \u001b[39min\u001b[39;00m possible_pair:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     \u001b[39mif\u001b[39;00m compute_Hc(spread_df[cnd])[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m hurst_threshold:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         hurst_screened_list\u001b[39m.\u001b[39mappend(cnd)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/Sun/Documents/Github/Statistical-Arbitrage/Project/Pair_Selection.ipynb#W2sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m spread_df \u001b[39m=\u001b[39m spread_df[hurst_screened_list]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/quant/lib/python3.8/site-packages/hurst/__init__.py:147\u001b[0m, in \u001b[0;36mcompute_Hc\u001b[0;34m(series, kind, min_window, max_window, simplified)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mCompute H (Hurst exponent) and C according to Hurst equation:\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mE(R/S) = c * T^H\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m    for further plotting log(data[0]) on X and log(data[1]) on Y\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(series)\u001b[39m<\u001b[39m\u001b[39m100\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSeries length must be greater or equal to 100\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m ndarray_likes \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mndarray]\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpandas.core.series\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mValueError\u001b[0m: Series length must be greater or equal to 100"
     ]
    }
   ],
   "source": [
    "price_df = pd.read_csv('./Data/us_etf_price.csv', parse_dates=['Date'])\n",
    "etf_info = pd.read_csv('./Data/etfs_details_equity.csv')\n",
    "\n",
    "start_date       = '2021-01-01'\n",
    "end_date         = '2021-03-31'\n",
    "test_start_date  = '2021-04-01'\n",
    "test_end_date    = '2021-04-30'\n",
    "\n",
    "ps = pair_select(price_df, etf_info, start_date, end_date, test_start_date, test_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('quant')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7b6214bfe3ce19b2336b5ee82fe9739f68968628bd5eb86b17deae08d37c2fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
